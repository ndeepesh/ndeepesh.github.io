<!DOCTYPE html>
<html lang="en">
<head prefix="og: http://ogp.me/ns# article: http://ogp.me/ns/article# website: http://ogp.me/ns/website#">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
    <meta name="description" content="">
    <meta property="og:title" content="tSNE">
    
    <meta property="og:type" content="article">
    <meta property="article:published_time" content="2017-07-05">
    
    <meta property="og:description" content="">
    <meta property="og:url" content="https://ndeepesh.github.io/post/tsne/">
    <meta property="og:site_name" content="Deepesh Nathani">
    
    <meta property="og:tags" content="Dimensionality Reduction">
    
    <meta property="og:tags" content="Python">
    
    <meta name="generator" content="Hugo 0.25-DEV" />
    <title>tSNE &middot; Deepesh Nathani</title>
    
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
    <link href="/css/prism.css" rel="stylesheet" />
    
    <link rel="stylesheet" href="https://ndeepesh.github.io//css/style.css">
    <link rel="stylesheet" href="https://ndeepesh.github.io//css/custom.css">
    
    <link href="https://ndeepesh.github.io/index.xml" rel="alternate" type="application/rss+xml" title="Deepesh Nathani" />
    
    
    <link rel="icon" href="https://ndeepesh.github.io///static/favicon.jpeg" />
    

    
    
</head>
<body>

<nav class="navbar navbar-default navbar-fixed-top visible-xs">
	<div class="container-fluid">
		<div class="navbar-header">
			<button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
				<span class="sr-only">Toggle navigation</span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
				<span class="icon-bar"></span>
			</button>
			
				<a class="navbar-brand" href="https://ndeepesh.github.io/">Deepesh Nathani</a>
			
		</div>
		<div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
			<ul class="nav navbar-nav">
				
				
					<li><a href="https://ndeepesh.github.io/about/">About</a></li>
				
			</ul>
		</div>
	</div>
</nav>
<div class="container-fluid">
	<div class="row">
		<div id="menu" class="hidden-xs col-sm-4 col-md-3">
	<div id="menu-content" class="vertical-align">
		
			<h1 class="text-center"><a href="https://ndeepesh.github.io/">Deepesh Nathani</a></h1>
		
		
		
			<small class="text-center center-block">Software Developer</small>
		
		
		
			<img id="profile-pic" src="https://ndeepesh.github.io///static/img/me.jpg" alt="My Picture" class="img-circle center-block">
		
		<div id="social" class="text-center">
			
				<a href="https://www.facebook.com/deepesh.nathani"><i class="fa fa-facebook fa-2x"></i></a>
			
				<a href="https://www.linkedin.com/in/deepeshnathani/"><i class="fa fa-linkedin fa-2x"></i></a>
			
				<a href="https://github.com/ndeepesh"><i class="fa fa-github fa-2x"></i></a>
			

			<a href="mailto:ndeep_27@yahoo.com"><i class="fa fa-envelope-o fa-2x"></i></a>
		</div>
		<div id="links" class="text-center">
			
			
				<a href="https://ndeepesh.github.io/about/">About</a>
			
		</div>
	</div>
</div>

		<div id="content" class="col-xs-12 col-sm-8 col-md-9">
			<div class="row">
				<div id="post" class="col-sm-offset-1 col-sm-10 col-md-10 col-lg-8">


<main>
	<header>
		<h1>tSNE</h1>
	</header>

	<article>
		<p>Recently, I came across an interesting non-linear visualization or dimensionality reduction method - tSNE(Distributed Stochastic Neighbour Embedding). <a href="http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf">Here</a> is the original paper describing the method in much more detail. I thought i&rsquo;ll implement this from scratch. In this post I am sharing the code with extensive comments :)</p>

<p>Before moving onto the code, here is a brief overview of how it works. tSNE is just like PCA where you can reduce the dimensionality of data to 2 or 3 dimensions to visualize it. But unlike PCA, tSNE does that using non-linear ways. It converts everything to probability distributions. Next I&rsquo;ll list down the steps on a high level for a bigger picture. <br/>
1. Assume we have points of shape=(n, m). We&rsquo;ll call this original or higher dimension<br/>
2. We need to convert these points to shape=(n, 2). We&rsquo;ll call this lower or embedded dimension.<br/>
3. Calculate Pairwise similarity matrix for all data points(example eculedian distance between all pairs)<br/>
4. Each point is then converted to a probability distribution using the above similarity matrix<br/>
5. Now, we initialize n points using a normal distribution but in the lower dimension.<br/>
6. Again we convert these new points to a probability distribution<br/>
7. Finally, run gradient-descent on the two distributions(point 4 and 6) with kl-divergence between them as the cost function <br/></p>

<p>Below is the code. Hope it helps. <br/></p>

<pre><code class="language-python"># Main Function Entry
def tSNE(data, req_dims=2, intermediate_dims=2, perplexity=20.0):
    (n, d) = data.shape
        
    # Calculate the Squared Pairwise Eucledian Distance Matrix
    eucledian_simi_matrix = euclidean_distances(data)**2
    assert(eucledian_simi_matrix.shape == (n, n))
    
    # beta = 1/2*square(variance) -&gt; Initialize beta's to 1 =&gt; variance = 1/sqrt(2)
    # Variance is of the Gaussian Distribution. Each point will have its own gaussian-distribution, thus n variances 
    beta = np.ones((n, 1))
    
    # Perplexity(of a distribution) = 2^entropy =&gt; entropy = log(perplexity)
    entropy = np.log(perplexity)
    
    # Initialize the Conditional Gaussian Distribution for all points as 0
    # p(i|j) = (e^(-||xi - xj||^2)/2*variance^2) / summation(e^(-||xi - xm||^2)/2*variance^2)
    # p(i|j) =&gt; Probability that point-i will choose point-j as its neighbour if points were selected from a 
    #           gaussian distribtion centered at point-i
    gaussian_cond_dis = np.zeros((n, n))
    
    # In this for loop we find optimal beta(thus variance) for each point's gaussian distribution
    # This is done using binary search. We are given a desired perplexity(and thus entropy)
    # What we do is first we calculate the entropy given our initialized beta(above) and see whether we need to
    # increase or decrease beta to reach the desired perplexity for each individual point
    for point in range(n):
        if(point % 100 == 0): print(&quot;Completed calculating Prob. Distributions of &quot; + str(point) + &quot; data points&quot;)
        
        # We remove the self-distance in eucledian matrix
        mofified_simi = eucledian_simi_matrix[point][np.r_[0:point, point+1:n]]
        point_entropy, prob_dis = calculateEntrAndProbDis(mofified_simi, beta[point])
        diff = point_entropy - entropy
        attempts = 0
        
        min_beta = -np.inf
        max_beta = np.inf
        
        # Here we do a Binary Search on Beta Values(Remember: beta = 1/2*sqrt(variance))
        # If difference &gt; 0 =&gt; Increase Beta else Decrease. this is done till some predefined number of attempts
        while(np.absolute(diff) &gt; 1e-5 and attempts &lt; 50):
            if(diff &gt; 0):
                min_beta = beta[point] # our new minimum for binary search
                if(max_beta == np.inf or max_beta == -np.inf):
                    beta[point] = beta[point] * 2
                else:
                    beta[point] = (beta[point] + max_beta)/2 #Somewhere between current and max
            else:
                max_beta = beta[point] # our new max for binary search
                if(min_beta == np.inf or min_beta == -np.inf):
                    beta[point] = beta[point] / 2
                else:
                    beta[point] = (beta[point] + min_beta)/2 #Between current and minimum

            attempts += 1
            
            # Do the drill again till we run out of attempts or we get a very good beta(thus variance)
            point_entropy, prob_dis = calculateEntrAndProbDis(mofified_simi, beta[point])
            diff = point_entropy - entropy

        gaussian_cond_dis[point][np.r_[0:point, point+1:n]] = prob_dis

    print(&quot;Mean Beta Value = &quot; + str(np.mean(np.sqrt(1/beta))))
    
    # Symmetric Joint Probability Matrix (For points in Higher Dimension). Symmetric SNE
    joint_prob_matrix = (gaussian_cond_dis + gaussian_cond_dis.T)/(2*n)
    assert(joint_prob_matrix.shape == (n, n))
    
    # Early exagerration here(this is a very important optimization step)
    joint_prob_matrix = joint_prob_matrix*4
    joint_prob_matrix = np.maximum(joint_prob_matrix, 1e-12);
    
    # Sample Initial Map Points from Normal Distribution with mean 0 and variance 10^-4
    map_pts = np.random.randn(n, req_dims)
    
    # We search for optimal map points by using a gradient descent on KL Divergence between probability distributions
    # of higher and lower dimension. Map Points are already initialized as per normal function above
    epocs = 1000
    learn_rate = 0.9
    momentum = 0.5
    learning_decaying_rate = 0.9
    
    moment = np.zeros((n, req_dims))
    all_grads_squared = np.zeros((n, req_dims))
    
    # Get Gradient w.r.t to each map_point
    change_y = np.zeros((n, req_dims))
    iY = np.zeros((n, req_dims))
    gains = np.ones((n, req_dims))
    eta = 500
    
    # Student Distribution Fucntion
    student_func = lambda x: np.power(1 + x**2, -1) # Lambda for Student-t distribution
    
    for i in range(epocs):
        sys.stdout.write(&quot;\rProgress: {:2.1f}&quot;.format(100 * i/float(epocs)))
        
        if(i &gt; 250): momentum = 0.8
            
        # Calculate the t-Distribution of map points
        tDistri = student_func(euclidean_distances(map_pts))
        np.fill_diagonal(tDistri, 0) # Again, only interested in pair-wise similarities
        sumTDist = np.sum(tDistri)
        joint_prob_t_distri = tDistri/sumTDist
        joint_prob_t_distri = np.maximum(joint_prob_t_distri, 1e-12)
        
        assert(joint_prob_t_distri.shape == (n, n))
        
        cost = np.sum(joint_prob_matrix*np.log(joint_prob_matrix/joint_prob_t_distri))
        if(i % 10 == 0): print(&quot; Cost after &quot; + str(i) + &quot; iterations = &quot; + str(cost))
        
        PQ = joint_prob_matrix - joint_prob_t_distri
        for j in range(n):
            # Gradient = 4*summation((pij - qij)*(yi- yj)*(1+||yi-yj||^2)^-1)
            diff_dis = np.reshape((PQ[j]), (n, 1))
            temp = diff_dis*np.reshape(tDistri[j], (n, 1))
            distances = map_pts[j] - map_pts
            change_y[j] = np.sum(temp*distances, 0)
            
        # Modify Map Points applying learn_rate, momentum and adaptive learning rates(Adam)
        all_grads_squared = learning_decaying_rate*all_grads_squared + (1 - learning_decaying_rate)*(change_y**2)
        moment = momentum*moment + (1-momentum)*change_y
        map_pts -= (learn_rate)/(np.sqrt(all_grads_squared) + 1e-8)*moment
        
        if(i == 100):
            joint_prob_matrix = joint_prob_matrix/4 # Remove effect of early exagerration
    
    assert(map_pts.shape == (n, req_dims))
    return map_pts

# Calculation here is a result of simplifying entropy and gaussian distribution equations
# Returns Entropy and Prob Distribution corresponding to that entropy for one point
def calculateEntrAndProbDis(D, beta):
    gaussian_dist = np.exp(-D.copy()*beta) # Numerator of Conditional Probability defined below
    sumProbDist = np.maximum(np.sum(gaussian_dist), 1e-10) # Denominator of Conditional Probability
    conditional_gaussian = gaussian_dist/sumProbDist

    entropy = np.sum(np.log(sumProbDist)*conditional_gaussian) + np.sum(conditional_gaussian*D)*beta
    return (entropy, conditional_gaussian)

</code></pre>

<p>Run tSNE on MNIST Data <br/></p>

<pre><code class="language-python">def scatter(x, colors):
    # We choose a color palette with seaborn.
    palette = np.array(sns.color_palette(&quot;hls&quot;, 10))

    # We create a scatter plot.
    f = plt.figure(figsize=(8, 8))
    ax = plt.subplot(aspect='equal')
    sc = ax.scatter(x[:,0], x[:,1], lw=0, s=40,
                    c=palette[colors.astype(np.int)])
    plt.xlim(-25, 25)
    plt.ylim(-25, 25)
    ax.axis('off')
    ax.axis('tight')

    # We add the labels for each digit.
    txts = []
    for i in range(10):
        # Position of each label.
        xtext, ytext = np.median(x[colors == i, :], axis=0)
        txt = ax.text(xtext, ytext, str(i), fontsize=24)
        txt.set_path_effects([
            PathEffects.Stroke(linewidth=5, foreground=&quot;w&quot;),
            PathEffects.Normal()])
        txts.append(txt)

    return f, ax, sc, txts

X = np.loadtxt(&quot;../data/mnist/mnist2500_X.txt&quot;)
labels = np.loadtxt(&quot;../data/mnist/mnist2500_labels.txt&quot;)
Y = tSNE(X)

scatter(Y, labels)
plt.show()
</code></pre>

<p><img src="../static/tSNE/output_8_0.png" alt="png" /></p>

	</article>
</main>

<div id="bottom-nav" class="text-center center-block">
	<a href=" https://ndeepesh.github.io/" class="btn btn-default"><i class="fa fa-home"></i> Home</a>
</div>


  <a href="#" id="disqus_load" onclick="return loadDisqus()">Add / View Comments</a>
<div id="disqus_thread"></div>

<script type="text/javascript">
  var disqus_shortname = 'ndeepesh-github-io';
  function loadDisqus() {
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  
  document.getElementById('disqus_load').style.display = 'none';
  return false;
}
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-98795273-1', 'auto');
  ga('send', 'pageview');

</script>

						</div>
					</div>
				</div>
			</div>
		</div>
  </div>
  
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-98795273-1', 'auto');
    ga('send', 'pageview');
    window.baseURL = "https:\/\/ndeepesh.github.io\/";
  </script>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.2/js/bootstrap.min.js"></script>
	<script type="text/x-mathjax-config">
  	MathJax.Hub.Config({
    	tex2jax: {
      	inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      	processEscapes: true
    	}
  	});
	</script>
	<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
  <script src="//cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js"></script>
  <script src="//cdnjs.cloudflare.com/ajax/libs/topojson/1.6.9/topojson.min.js"></script>
  
  
  <script src="https://ndeepesh.github.io//js/App.js"></script>
  
</body>
<script src="https://ndeepesh.github.io//js/prism.js"></script>
</html>

